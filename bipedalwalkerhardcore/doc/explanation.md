## 目的



## ランダム操作
とりあえず動作確認。
![alt text](image.png)

## この問題の特徴
BipedalWalkerHardcoreは、連続値の行動空間を持つ高難易度のロボティクス環境であり、ノイズや不安定さ、複雑な報酬設計が特徴です。
- 連続値の行動空間
- ノイズや不安定さ
- 複雑な報酬設計

## 強化学習フレームワークの検討

そのため、以下の強化学習アルゴリズムが有力です。


### 1. **Proximal Policy Optimization (PPO)**
**選定理由:**  
- サンプル効率が高く、安定した学習が可能。
- 連続制御タスクで実績が多く、BipedalWalkerHardcoreでも多くの成功例が報告されています。
- ハイパーパラメータの調整が比較的容易で、実装も豊富。

---

### 2. **Soft Actor-Critic (SAC)**
**選定理由:**  
- 最大エントロピー強化学習により、探索性が高く、多様な状況での学習が安定。
- サンプル効率が高く、ノイズの多い環境でもロバストに学習できる。
- 連続行動空間のタスクで最先端の性能を発揮している。

---

### 3. **Twin Delayed DDPG (TD3)**
**選定理由:**  
- DDPGの改良版で、過推定バイアスを抑制し、より安定した学習が可能。
- 連続制御タスクでPPOやSACと並ぶ高い性能を示している。
- ノイズ耐性が高く、BipedalWalkerHardcoreのような難易度の高い環境にも適している。

---

### 4. **Deep Deterministic Policy Gradient (DDPG)**
**選定理由:**  
- 連続行動空間に特化したアクター・クリティック型アルゴリズム。
- 探索ノイズの導入や経験再生バッファによる効率的な学習が可能。
- TD3の登場以降はやや古典的だが、十分な性能を発揮できる。

---

### 5. **Distributed Distributional DDPG (D4PG)**
**選定理由:**  
- DDPGを分散並列化し、分布的価値推定を導入した発展型。
- 大規模並列学習と分布的強化学習による高い性能。
- より複雑な環境や大規模なタスクにも適用可能。




