# マルチエージェント in 深層強化学習

## 概要

本日はActor-Critic手法として有名なDDPG(Deep Deterministic Policy Gradient)を拡張した手法である[MADDPG(Multi-Agent Deep Deterministic Policy Gradient)](https://arxiv.org/pdf/1706.02275.pdf)について紹介

強化学習は単一エージェントに対して発展してきたが、現実世界ではエージェントは複数存在する。

より効率的に仕事をさせるためにはエージェントを協調的に制御する必要がある。(例. 工場内のUAVとUGVの協調作業など)

MARLは近年では盛んに研究されていて、単一エージェントと比べて、行動や学習はとても難しい。

エージェントが増加すれば増加するほど、学習する際の勾配が正しい方向に進む確率が指数関数的に低下する。

## 全体イメージ

イメージは以下の図のようになる。

N個のエージェントはそれぞれ方策 $\pi = {\pi_1, \pi_2 ... \pi_N}$とします。

状態値 $o_i$ をそれぞれの方策に渡すとエージェントが取るべき行動 $o_i$が出力される。

これはそれぞれのエージェント(actor)は **Decentralized** (分散)な方策をとることを意味しています。

次にすべてのエージェントを管理できるN個の価値関数(Q関数）があります。

この価値関数はすべてのエージェントから状態値 $x={o_1,...o_N}$と行動 $a={a_1,...a_N}$ を受け取ってその価値を返します。

言い換えると**Centralized**なcritic(批評家)であることを意味しています。

つまり、N人のcriticがすべてのactor(演者)について評価し、actorたちはその評価に応じてそれぞれのとる方策を更新します。

![1749549270736](image/1-what-multi-agent/1749549270736.png)




## 参考

[マルチエージェント深層強化学習を勉強しよう　第一回-MADDPGの解説及び実装 #Python - Qiita](https://qiita.com/mayudong200333/items/4a09a52e58a66a766ab2)
