対数確率（log-probabilities）とは、**確率の値に対して対数（log）を取ったもの**です。  
通常、自然対数（底がe）を使います。

---

## 1. **定義**

- ある事象の確率 \( p \) に対して、その対数確率は \( \log(p) \) です。
- 例: \( p = 0.2 \) のとき、対数確率は \( \log(0.2) \approx -1.609 \)

---

## 2. **なぜ使うのか？**

### (1) 数値的安定性
- 確率は0〜1の間の値なので、複数の確率を掛け合わせると非常に小さな値（アンダーフロー）になりやすい。
- 対数を取ると掛け算が足し算に変わるので、計算が安定する。
  - 例: \( p_1 \times p_2 \times p_3 \) → \( \log(p_1) + \log(p_2) + \log(p_3) \)

### (2) 損失関数で使いやすい
- 機械学習では、クロスエントロピー損失や負の対数尤度（Negative Log Likelihood, NLL）など、対数確率を使う損失関数が多い。
- これは「正解の確率が高いほど損失が小さくなる」性質を持つ。

### (3) 微分・最適化がしやすい
- 対数確率を使うと、損失関数の勾配計算が簡単になり、効率的に学習できる。

---

## 3. **強化学習での使い方**

- 方策勾配法（REINFORCEなど）では、「選択した行動の確率の対数（log-prob）」を損失関数に使う。
- 例:  
  \[
  \text{損失} = -\log(\pi(a|s)) \times \text{報酬}
  \]
  ここで \(\pi(a|s)\) は状態\(s\)で行動\(a\)を選ぶ確率。

---

## 4. **プログラム例（PyTorch）**

```python
probs = torch.tensor([0.2, 0.5, 0.3])
m = torch.distributions.Categorical(probs)
action = m.sample()
log_prob = m.log_prob(action)  # これが「対数確率」
```

---

## 5. **特徴まとめ**

- 対数確率は通常負の値になる（確率が1未満の場合）。
- 対数確率を使うことで、計算の安定性・効率が向上する。
- 機械学習・深層学習の多くの場面で標準的に用いられる。

---

以上です。